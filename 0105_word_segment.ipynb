{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     1,
     11,
     31,
     44,
     57,
     68,
     76,
     85,
     116
    ]
   },
   "outputs": [],
   "source": [
    "# 加载停用词表\n",
    "def load_stopwords():\n",
    "    path = './stopwords/final_stopwords.txt'\n",
    "    f = open(path,encoding='utf-8')\n",
    "    stopwords = set()\n",
    "    for line in f:\n",
    "        stopwords.add(line.strip())\n",
    "    f.close()\n",
    "    return stopwords\n",
    "\n",
    "# 预处理句子\n",
    "def handle_text(text):\n",
    "    text = str(text) #强制转为字符串\n",
    "    #转换罗马字母为中文数字，不转阿拉伯数字是为了避免分割\n",
    "    text = re.sub(r'Ⅲ|III','三',text)\n",
    "    text = re.sub(r'Ⅱ|II','二',text)\n",
    "    text = re.sub(r'Ⅳ','四',text)\n",
    "    text = re.sub(r'3级','三级',text)\n",
    "    text = re.sub(r'2级','二级',text)\n",
    "    text = re.sub(r'1级','一级',text)\n",
    "    text = re.sub(r'2型','二型',text)\n",
    "    text = re.sub(r'1型','一型',text)\n",
    "    text = re.sub(r'[I1]期','一期',text)\n",
    "    #把小写字母转成大写字母\n",
    "    text = text.upper()\n",
    "    #所有的非中文字符和大写字母的用\"#\"替代作为分隔符\n",
    "    not_chinese = re.compile(r'[^\\u4e00-\\u9fa5A-Z-]+')\n",
    "    text = re.sub(not_chinese,'#',text) \n",
    "    return text\n",
    "\n",
    "# 从以\"#\"连接的文本中提取专有名词\n",
    "def extra_dict(text):\n",
    "    ans = set()\n",
    "    #提取出“级”和“型”和其前面的数字或者字母单独拿出来 (用于扩充词典)\n",
    "    #把字母形成的单元拿出来用于扩充词典\n",
    "    pattern1 = re.compile(r'[\\d|A-z]+[级|型|期]')\n",
    "    pattern2 = re.compile(r'[A-Z]+[-]?[A-Z]+')\n",
    "    lis = pattern1.findall(text)\n",
    "    lis.extend(pattern2.findall(text))\n",
    "    for w in lis:\n",
    "        ans.add(w)\n",
    "    return ans\n",
    "\n",
    "# 分词后对单词再做一些合并处理,输入以分词列表的形式\n",
    "def combine_word(w_list,anchor):\n",
    "    length = len(w_list)\n",
    "    if length <= 1:\n",
    "        return w_list\n",
    "    ans = [w_list[0]]\n",
    "    for i in range(1,length):\n",
    "        if w_list[i] == anchor:\n",
    "            ans.append(ans.pop(-1) + w_list[i])\n",
    "        else:\n",
    "            ans.append(w_list[i])\n",
    "    return ans\n",
    "\n",
    "# 分词以\"#\"连接的文本\n",
    "def cut(text,stopwords):\n",
    "    ans = []\n",
    "    for item in text.split('#'):\n",
    "        if item != '':\n",
    "            ans.extend(jieba.cut(item,use_paddle=False))\n",
    "    # 处理分词后的单词\n",
    "    ans = combine_word(ans,'术')\n",
    "    ans = combine_word(ans,'段')\n",
    "    return '|'.join([w for w in ans if w not in stopwords])\n",
    "\n",
    "# 往jieba库中添加或者删除以集合表示的词语扩展词典\n",
    "def add_word(word_set):\n",
    "    if len(word_set) > 0:\n",
    "        for w in word_set:\n",
    "            jieba.add_word(w,1000000)\n",
    "        pprint('添加词语到词典成功')\n",
    "    else:\n",
    "        pprint('没有词语可以添加到词典')\n",
    "            \n",
    "def remove_word(word_set):\n",
    "    if len(word_set) > 0:\n",
    "        for w in word_set:\n",
    "            jieba.del_word(w)\n",
    "        pprint('从词典删除词语成功')\n",
    "    else:\n",
    "        pprint('没有词语可以删除')\n",
    "        \n",
    "# 分词 返回一个列表\n",
    "def cutall(corpus):\n",
    "    ans = []\n",
    "    # 初始化专有名词词典\n",
    "    extracted_dict.clear()\n",
    "\n",
    "    # 构建词典\n",
    "    for text in corpus:\n",
    "        s = handle_text(text)\n",
    "        tmp_dict = extra_dict(s)\n",
    "        if len(tmp_dict) > 0:\n",
    "            # 添加专有名词到词典中\n",
    "            for w in tmp_dict:\n",
    "                extracted_dict.add(w)\n",
    "    pprint('构建的词典长度：' + str(len(extracted_dict)))\n",
    "    # 分词\n",
    "    add_word(extracted_dict)\n",
    "    try:\n",
    "\n",
    "        for text in corpus:\n",
    "            s = handle_text(text)\n",
    "            ans.append(cut(s,stopwords))\n",
    "#             pprint(text + '--->'+cut(s,stopwords))\n",
    "        #删除词典\n",
    "        remove_word(extracted_dict)\n",
    "    except:\n",
    "        pprint('分词出错')\n",
    "        remove_word(extracted_dict)\n",
    "    pprint('分词后的文本长度：' + str(len(ans)))\n",
    "    return ans\n",
    "\n",
    "# 加载初始的人工词典--用来添加算法无法发现的词语\n",
    "def add_manual_dict(manual_dict):\n",
    "    for w in manual_dict:\n",
    "        jieba.add_word(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\xwd\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.719 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 使用第三方医疗词典\n",
    "jieba.load_userdict('./med_dict/med_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3629, 28)\n",
      "停用词表长度:2126\n"
     ]
    }
   ],
   "source": [
    "manual_dict = {'造瘘术'}\n",
    "\n",
    "path = './data/data_without_na_01.xlsx'\n",
    "df = pd.read_excel(path)\n",
    "pprint(df.shape)\n",
    "\n",
    "# 手动扩展词典\n",
    "add_manual_dict(manual_dict)\n",
    "\n",
    "# 加载停用词\n",
    "stopwords = load_stopwords()\n",
    "print('停用词表长度:' + str(len(stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_diagnose_all = df['术前诊断']\n",
    "corpus_opearation_all = df['实施手术']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'构建的词典长度：48'\n",
      "'添加词语到词典成功'\n",
      "'从词典删除词语成功'\n",
      "'分词后的文本长度：3629'\n"
     ]
    }
   ],
   "source": [
    "# 分词术前诊断\n",
    "extracted_dict = set()\n",
    "diagnose_cut = cutall(corpus_diagnose_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A型',\n",
      " 'A期',\n",
      " 'BENTALL',\n",
      " 'B期',\n",
      " 'CA',\n",
      " 'CIN',\n",
      " 'CINI',\n",
      " 'CKD',\n",
      " 'CM',\n",
      " 'CUSHING',\n",
      " 'DDH',\n",
      " 'ESD',\n",
      " 'FNH',\n",
      " 'FREIBERG',\n",
      " 'GDM',\n",
      " 'GGN',\n",
      " 'GGO',\n",
      " 'GNRH',\n",
      " 'GS',\n",
      " 'HELIP',\n",
      " 'HISL',\n",
      " 'HPV',\n",
      " 'IB',\n",
      " 'IB期',\n",
      " 'ICP',\n",
      " 'IGA',\n",
      " 'IUD',\n",
      " 'IV',\n",
      " 'IVF-ET',\n",
      " 'IV级',\n",
      " 'LISFRANCE',\n",
      " 'LST',\n",
      " 'MD',\n",
      " 'OB',\n",
      " 'PCI',\n",
      " 'PCNL',\n",
      " 'PDA',\n",
      " 'PSA',\n",
      " 'RATHKE',\n",
      " 'RH',\n",
      " 'RMCA',\n",
      " 'RP',\n",
      " 'STANFORD',\n",
      " 'TACE',\n",
      " 'TFCC',\n",
      " 'TIA',\n",
      " 'VIN',\n",
      " 'WG'}\n"
     ]
    }
   ],
   "source": [
    "pprint(extracted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'构建的词典长度：26'\n",
      "'添加词语到词典成功'\n",
      "'从词典删除词语成功'\n",
      "'分词后的文本长度：3629'\n"
     ]
    }
   ],
   "source": [
    "# 分词实施手术\n",
    "opearation_cut = cutall(corpus_opearation_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACCF',\n",
      " 'ACDF',\n",
      " 'BENTALL',\n",
      " 'CAGE',\n",
      " 'DIXON',\n",
      " 'DSA',\n",
      " 'ENBLOC',\n",
      " 'IABP',\n",
      " 'MED',\n",
      " 'MIS',\n",
      " 'MIS-TLIF',\n",
      " 'O-LIF',\n",
      " 'PEID',\n",
      " 'PELD',\n",
      " 'PKP',\n",
      " 'PKRP',\n",
      " 'PLIF',\n",
      " 'PVP',\n",
      " 'ROUXY',\n",
      " 'TATME',\n",
      " 'TLIF',\n",
      " 'TURP',\n",
      " 'VSD',\n",
      " 'WHIPPLE',\n",
      " 'Z-TLIF',\n",
      " 'ZELIF'}\n"
     ]
    }
   ],
   "source": [
    "pprint(extracted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dataAnalysis]",
   "language": "python",
   "name": "conda-env-dataAnalysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
